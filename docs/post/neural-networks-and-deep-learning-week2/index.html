<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8" />

  
  <title>Neural Networks and Deep Learning, Week 2</title>

  
  





  
  <meta name="author" content="Alexander Wong" />
  <meta name="description" content="Taking the Coursera Deep Learning Specialization, Neural Networks and Deep Learning course. Will post condensed notes every week as part of the review process. All material originates from the free Coursera course, taught by Andrew Ng. See deeplearning.ai for more details.
Assumes you have knowledge of Week 1.
Table of Contents  Neural Networks Basics  Logistic Regression as a Neural Network  Binary Classification Logistic Regression Logistic Regression Cost Function Gradient Descent Derivatives More Derivatives Examples Computation Graph Derivatives with a Computation Graph Logistic Regression Gradient Descent Gradient Descent on m Examples  Python and Vectorization  Vectorization More Vectorization Examples Vectorizing Logistic Regression Vectorizing Logistic Regression&amp;rsquo;s Gradient Output Broadcasting in Python Note on Python/NumPy Vectors     Neural Networks Basics Logistic Regression as a Neural Network Binary Classification Binary classification is basically answering a yes or no question." />

  
  
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@FindingUdia" />
    <meta name="twitter:title" content="Neural Networks and Deep Learning, Week 2" />
    <meta name="twitter:description" content="Taking the Coursera Deep Learning Specialization, Neural Networks and Deep Learning course. Will post condensed notes every week as part of the review process. All material originates from the free Coursera course, taught by Andrew Ng. See deeplearning.ai for more details.
Assumes you have knowledge of Week 1.
Table of Contents  Neural Networks Basics  Logistic Regression as a Neural Network  Binary Classification Logistic Regression Logistic Regression Cost Function Gradient Descent Derivatives More Derivatives Examples Computation Graph Derivatives with a Computation Graph Logistic Regression Gradient Descent Gradient Descent on m Examples  Python and Vectorization  Vectorization More Vectorization Examples Vectorizing Logistic Regression Vectorizing Logistic Regression&amp;rsquo;s Gradient Output Broadcasting in Python Note on Python/NumPy Vectors     Neural Networks Basics Logistic Regression as a Neural Network Binary Classification Binary classification is basically answering a yes or no question." />
    <meta name="twitter:image" content="https://alexander-wong.com/img/avatar.jpg" />
  




<meta name="generator" content="Hugo 0.31.1" />


<link rel="canonical" href="https://alexander-wong.com/post/neural-networks-and-deep-learning-week2/" />
<link rel="alternative" href="https://alexander-wong.com/index.xml" title="Alexander Wong" type="application/atom+xml" />


<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<meta name="format-detection" content="telephone=no,email=no,adress=no" />
<meta http-equiv="Cache-Control" content="no-transform" />


<meta name="robots" content="index,follow" />
<meta name="referrer" content="origin-when-cross-origin" />







<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="apple-mobile-web-app-title" content="Alexander Wong" />
<meta name="msapplication-tooltip" content="Alexander Wong" />
<meta name='msapplication-navbutton-color' content="#5fbf5e" />
<meta name="msapplication-TileColor" content="#5fbf5e" />
<meta name="msapplication-TileImage" content="/img/tile-image-windows.png" />
<link rel="icon" href="https://alexander-wong.com/img/favicon.ico" />
<link rel="icon" type="image/png" sizes="16x16" href="https://alexander-wong.com/img/favicon-16x16.png" />
<link rel="icon" type="image/png" sizes="32x32" href="https://alexander-wong.com/img/favicon-32x32.png" />
<link rel="icon" sizes="192x192" href="https://alexander-wong.com/img/touch-icon-android.png" />
<link rel="apple-touch-icon" href="https://alexander-wong.com/img/touch-icon-apple.png" />
<link rel="mask-icon" href="https://alexander-wong.com/img/safari-pinned-tab.svg" color="#5fbf5e" />



<link rel="stylesheet" href="//cdn.bootcss.com/video.js/6.2.8/alt/video-js-cdn.min.css" />

<link rel="stylesheet" href="https://alexander-wong.com/css/bundle.css" />


  
  <!--[if lt IE 9]>
    <script src="//cdn.bootcss.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="//cdn.bootcss.com/respond.js/1.4.2/respond.min.js"></script>
    <script src="//cdn.bootcss.com/video.js/6.2.8/ie8/videojs-ie8.min.js"></script>
  <![endif]-->

<!--[if lte IE 11]>
    <script src="//cdn.bootcss.com/classlist/1.1.20170427/classList.min.js"></script>
  <![endif]-->


<script src="//cdn.bootcss.com/object-fit-images/3.2.3/ofi.min.js"></script>


<script src="//cdn.bootcss.com/smooth-scroll/12.1.4/js/smooth-scroll.polyfills.min.js"></script>


</head>
  <body>
    
    <div class="suspension">
      <a title="Go to top" class="to-top is-hide"><span class="icon icon-up"></span></a>
      
        
      
    </div>
    
    
  <header class="site-header">
  <img class="avatar" src="https://alexander-wong.com/img/avatar.png" alt="Avatar">
  
  <h2 class="title">Alexander Wong</h2>
  
  <p class="subtitle">Incremental iterations towards meaning in life.</p>
  <button class="menu-toggle" type="button">
    <span class="icon icon-menu"></span>
  </button>
  <nav class="site-menu collapsed">
    <h2 class="offscreen">Main Menu</h2>
    <ul class="menu-list">
      
      
      
      
        <li class="menu-item
            
            
            ">
            <a href="https://alexander-wong.com/">Blog</a>
          </li>
      
        <li class="menu-item
            
            
            ">
            <a href="https://alexander-wong.com/about/">About</a>
          </li>
      
        <li class="menu-item
            
            
            ">
            <a href="https://alexander-wong.com/projects/">Projects</a>
          </li>
      
        <li class="menu-item
            
            
            ">
            <a href="https://alexander-wong.com/chatbot/">Chatbot</a>
          </li>
      
    </ul>
  </nav>
  <nav class="social-menu collapsed">
    <h2 class="offscreen">Social Networks</h2>
    <ul class="social-list">

      
      <li class="social-item">
        <a href="mailto:admin@alexander-wong.com" title="Email"><span class="icon icon-email"></span></a>
      </li>

      
      <li class="social-item">
        <a href="//github.com/awwong1" title="GitHub"><span class="icon icon-github"></span></a>
      </li>

      <li class="social-item">
        <a href="//twitter.com/FindingUdia" title="Twitter"><span class="icon icon-twitter"></span></a>
      </li>

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <li class="social-item">
        <a href="//www.linkedin.com/in/awwong1" title="Linkedin"><span class="icon icon-linkedin"></span></a>
      </li>

      

      

      

      <li class="social-item">
        <a href="https://alexander-wong.com/index.xml"><span class="icon icon-rss" title="RSS"></span></a>
      </li>

    </ul>
  </nav>
</header>

  <section class="main post-detail">
    <header class="post-header">
      <h1 class="post-title">Neural Networks and Deep Learning, Week 2</h1>
      <p class="post-meta">@Alexander Wong · Nov 18, 2017 · 7 min read</p>
    </header>
    <article class="post-content">

<p>Taking the <a href="https://www.coursera.org/specializations/deep-learning">Coursera Deep Learning Specialization</a>, <strong>Neural Networks and Deep Learning</strong> course. Will post condensed notes every week as part of the review process. All material originates from the free Coursera course, taught by <a href="http://www.andrewng.org/">Andrew Ng</a>. See <a href="https://www.deeplearning.ai/">deeplearning.ai</a> for more details.</p>

<p>Assumes you have knowledge of <a href="https://alexander-wong.com/post/neural-networks-and-deep-learning-week1/">Week 1</a>.</p>

<h1>Table of Contents</h1>
<nav id="TableOfContents">
<ul>
<li><a href="#neural-networks-basics">Neural Networks Basics</a>
<ul>
<li><a href="#logistic-regression-as-a-neural-network">Logistic Regression as a Neural Network</a>
<ul>
<li><a href="#binary-classification">Binary Classification</a></li>
<li><a href="#logistic-regression">Logistic Regression</a></li>
<li><a href="#logistic-regression-cost-function">Logistic Regression Cost Function</a></li>
<li><a href="#gradient-descent">Gradient Descent</a></li>
<li><a href="#derivatives">Derivatives</a></li>
<li><a href="#more-derivatives-examples">More Derivatives Examples</a></li>
<li><a href="#computation-graph">Computation Graph</a></li>
<li><a href="#derivatives-with-a-computation-graph">Derivatives with a Computation Graph</a></li>
<li><a href="#logistic-regression-gradient-descent">Logistic Regression Gradient Descent</a></li>
<li><a href="#gradient-descent-on-m-examples">Gradient Descent on m Examples</a></li>
</ul></li>
<li><a href="#python-and-vectorization">Python and Vectorization</a>
<ul>
<li><a href="#vectorization">Vectorization</a></li>
<li><a href="#more-vectorization-examples">More Vectorization Examples</a></li>
<li><a href="#vectorizing-logistic-regression">Vectorizing Logistic Regression</a></li>
<li><a href="#vectorizing-logistic-regression-s-gradient-output">Vectorizing Logistic Regression&rsquo;s Gradient Output</a></li>
<li><a href="#broadcasting-in-python">Broadcasting in Python</a></li>
<li><a href="#note-on-python-numpy-vectors">Note on Python/NumPy Vectors</a></li>
</ul></li>
</ul></li>
</ul>
</nav>


<h1 id="neural-networks-basics">Neural Networks Basics</h1>

<h2 id="logistic-regression-as-a-neural-network">Logistic Regression as a Neural Network</h2>

<h3 id="binary-classification">Binary Classification</h3>

<p>Binary classification is basically answering a yes or no question. For example: Is this an image of a cat? (1: Yes, 0: No).</p>

<p>Let&rsquo;s say you have an image of a cat that is 64 by 64 pixels. You have labeled training data indicating whether or not each image is a cat (<code>y=1</code>) or not a cat (<code>y=0</code>).</p>

<p><strong>Notation</strong></p>

<p>Let&rsquo;s say each picture can be represented as a single vector of size $n_x$ combined by joining three vectors (64 * 64 red pixel values) + (64 * 64 green pixel values) + (64 * 64 blue pixel values).</p>

<p>$$ n_x = \text{ unrolled image vector size } = 12288 $$
$$ m \text{ training examples } = \{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \dots,  (x^{(m)}, y^{(m)}) \} $$
$$ x \in \mathbb{R}^{n_x}, y \in \{ 0, 1 \} $$
$$ X \in \mathbb{R}^{n_x \times m} $$
$$ Y \in \mathbb{R}^{1 \times m} $$
$$ X = \begin{bmatrix} \vdots &amp; \vdots &amp; \vdots &amp; \vdots \newline x^{(1)} &amp; x^{(2)} &amp; \dots &amp; x^{(m)} \newline \vdots &amp; \vdots &amp; \vdots &amp; \vdots \end{bmatrix}  $$
$$ Y = \begin{bmatrix} y^{(1)} &amp; y^{(2)} &amp; \dots y^{(m)} \end{bmatrix} $$</p>

<h3 id="logistic-regression">Logistic Regression</h3>

<p>Logistic regression is when you want to have an answer in a continuous output. For instance, with the image of a cat problem, rather than having whether or not the image is of a cat or not, one could ask &ldquo;What is the probability that this is a cat?&rdquo;</p>

<p><strong>Notation</strong></p>

<p>Given $x$, want $\hat{y} = P(y=1 | x)$,
$$ x \in \mathbb{R}^{n_x} $$
$$ 0 \leq \hat{y} \leq 1 $$
$$ \text{Parameters: } w \in \mathbb{R}^{n_x}, b \in \mathbb{R} $$
$$ \text{Output: } \hat{y} = \sigma(w^Tx + b) $$
$$ z = w^Tx + b $$
$$ \sigma(z) = \dfrac{1}{1 + e^{-z}}  $$</p>

<p>If $z$ is a large positive number then $\sigma(z) = \dfrac{1}{1 + 0} \approx 1 $.</p>

<p>If $z$ is a large negative number then $\sigma(z) = \dfrac{1}{1 + \inf} \approx 0 $.</p>

<h3 id="logistic-regression-cost-function">Logistic Regression Cost Function</h3>

<p>A loss function is applied to a single training example. For logistic regression, typical loss function used is:</p>

<p>$$ \mathcal{L}(\hat{y}, y) = -(y\log{\hat{y}} + (1-y)\log{(1-\hat{y})}) $$</p>

<ul>
<li>If $y = 1$; $ \mathcal{L}(\hat{y}, y) = -\log{\hat{y}} $

<ul>
<li>Want $\log{\hat{y}}$ to be large, we want $\hat{y}$ to be large.</li>
</ul></li>
<li>If $y = 0$; $ \mathcal{L}(\hat{y}, y) = -\log{(1-\hat{y})}$

<ul>
<li>Want $\log{(1-\hat{y})}$ to be large, we want $\hat{y}$ to be small.</li>
</ul></li>
</ul>

<p>A cost function is applied to the entire training set, it evaluates the parameters of your algorithm. (Cost of your parameters).</p>

<p>$$ J(w, b) = \dfrac{1}{m} \sum\limits^{m}_{i=1} \mathcal{L}(\hat{y}^{(i)}, y^{(i)}) $$
$$ J(w, b) = -[\dfrac{1}{m} \sum\limits^{m}_{i=1} y^{(i)} \log{\hat{y}^{(i)}} + (1-y^{(i)})\log{(1-\hat{y}^{(i)})}] $$</p>

<h3 id="gradient-descent">Gradient Descent</h3>

<p>The cost function measures how well $w, b$ measure the training set. We want to find the $w, b$ that minimize $J(w, b)$.</p>

<p>Repeat {
  $$ w := w - \alpha \dfrac{\partial J(w, b)}{\partial w} $$
  $$ b := b - \alpha \dfrac{\partial J(w, b)}{\partial b} $$
}</p>

<p>Typically, in code, the derivative term is written as <code>dw</code>. Example: <code>w = w - alpha * dw</code>.</p>

<h3 id="derivatives">Derivatives</h3>

<p>You don&rsquo;t need a lot of calculus to understand neural networks. This is a basic example of the derivative of a straight line $f(a) = 3a$.</p>

<p><img src="https://alexander-wong.com/img/deeplearning-ai/intuition_about_derivatives.png" alt="intuition_about_derivatives" /></p>

<h3 id="more-derivatives-examples">More Derivatives Examples</h3>

<p>This is another example of the derivative of $f(a) = a^2$.</p>

<p><img src="https://alexander-wong.com/img/deeplearning-ai/intuition_about_derivatives_2.png" alt="intuition_about_derivatives_2" /></p>

<p>Here are three examples: $f(a) = a^2$, $f(a) = a^3$, and $f(a) = \log{a}$.</p>

<p><img src="https://alexander-wong.com/img/deeplearning-ai/intuition_about_derivatives_3.png" alt="intuition_about_derivatives_3" /></p>

<p>Take home:
- Derivative just means the slope of the line.
- You want to find slope? Look at calculus textbook.</p>

<h3 id="computation-graph">Computation Graph</h3>

<p>Computation graph is a left to right pass visualization of the math behind your algorithm.</p>

<p><img src="https://alexander-wong.com/img/deeplearning-ai/computation_graph_example.png" alt="computation_graph_example" /></p>

<h3 id="derivatives-with-a-computation-graph">Derivatives with a Computation Graph</h3>

<p>Recall calculus, chain rule.</p>

<p><img src="https://alexander-wong.com/img/deeplearning-ai/derivatives_computation_graph.png" alt="derivatives_computation_graph" />)</p>

<h3 id="logistic-regression-gradient-descent">Logistic Regression Gradient Descent</h3>

<p>Recall the follwing logistic regression formula defined above.</p>

<p><img src="https://alexander-wong.com/img/deeplearning-ai/logistic_regression_formula_recap.png" alt="logistic_regression_formula_recap" />
<img src="https://alexander-wong.com/img/deeplearning-ai/logistic_regression_formula_derivatives.png" alt="logistic_regression_formula_derivatives" /></p>

<h3 id="gradient-descent-on-m-examples">Gradient Descent on m Examples</h3>

<p>Recall the cost function:</p>

<p>$$ J(w, b)  = \dfrac{1}{m} \sum\limits^m_{i=1} \mathcal{L}(a^{(i)},y^{(i)}) $$
$$ a^{(i)} = \hat{y}^{(i)} = \sigma(z^{(i)}) = \sigma(w^Tx^{(i)} + b) $$</p>

<p>This is the naive formula for a single step of logistic regression on $m$ examples with $n = 2$ (two features) using gradient descent.</p>

<p><code>begin single step of gradient descent</code></p>

<p>$ J = 0; dw_1 = 0; dw_2 = 0; db = 0 $
<code>// define accumulator values</code></p>

<p>For $i = 1 \text{ to } m$ do {
  $$ z^{(i)} = w^Tx^{(i)} + b $$
  $$ a^{(i)} = \sigma(z^{(i)}) $$
  $$ J = - [ y^{(i)} \log(a^{(i)}) + (1-y^{(i)})\log(1-a^{(i)}) ] $$
  $$ dz^{(i)} = a^{(i)} - y^{(i)} $$
  $$ dw_1 = dw_1 + x_1^{(i)} \times dz^{(i)} $$
  $$ dw_2 = dw_2 + x_2^{(i)} \times dz^{(i)} $$
  <code>// if n were greater than two, continue to do this for dw_3, etc</code>
  $$ db = db + dz^{(i)} $$
}</p>

<p>$ J = \dfrac{J}{m} $;
$ dw_1 = \dfrac{dw_1}{m} $;
$ dw_2 = \dfrac{dw_2}{m} $;
$ db = \dfrac{db}{m} $;</p>

<p><code>end single step of gradient descent</code></p>

<p>For each step of gradient descent, you need to do effectively two for loops:</p>

<ol>
<li>for your $m$ number of training examples</li>
<li>for your $n$ number of example features.</li>
</ol>

<p>This is why vectorization is important in deep learning.</p>

<h2 id="python-and-vectorization">Python and Vectorization</h2>

<h3 id="vectorization">Vectorization</h3>

<p>Vectorization is the art of getting rid of explicit for loops in code.</p>

<p>Example: $ z = w^Tx + b $ where $ w \in \mathbb{R}^{n_x} $ and $ x \in \mathbb{R}^{n_x} $</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="o">//</span> <span class="n">non</span> <span class="n">vectorized</span>
<span class="n">z</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="n">x</span><span class="p">):</span>
  <span class="n">z</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">z</span> <span class="o">+=</span> <span class="n">b</span>

<span class="o">//</span><span class="n">vectorized</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span></code></pre></div>
<p>The following is a vectorization demo.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span>

<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;vectorized version: &#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">1000</span> <span class="o">*</span> <span class="p">(</span><span class="n">toc</span><span class="o">-</span><span class="n">tic</span><span class="p">))</span> <span class="o">+</span> <span class="sa"></span><span class="s2">&#34;ms&#34;</span><span class="p">)</span>
<span class="c1"># vectorized version: 14.4419670105ms</span>
<span class="n">c</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000000</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;non-vectorized version: &#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">1000</span> <span class="o">*</span> <span class="p">(</span><span class="n">toc</span><span class="o">-</span><span class="n">tic</span><span class="p">))</span> <span class="o">+</span> <span class="sa"></span><span class="s2">&#34;ms&#34;</span><span class="p">)</span>
<span class="c1"># non-vectorized version: 428.48610878ms</span></code></pre></div>
<p>Vectorization increases performance by allowing the program to take advantage of parallelization. Wherever possible, avoid for loops.</p>

<h3 id="more-vectorization-examples">More Vectorization Examples</h3>

<p>Whenever possible, avoid explicit for-loops.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># matrix times a vector, vectorized</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

<span class="c1"># apply exponential operation on every element of a matrix/vector</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">u</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="c1"># or vectorized</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="c1"># element wise log</span>
<span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="c1"># elementwise abs</span>
<span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># ReLU</span></code></pre></div>
<h3 id="vectorizing-logistic-regression">Vectorizing Logistic Regression</h3>

<p>We want to calculate:</p>

<p>for i in range of 1 to m {
  $$ z^{(i)} = w^Tx^{(i)} + b $$
  $$ a^{(i)} = \sigma(z^{(i)}) $$
}</p>

<p>Recall that $X$ is in the shape of $(n_x, m)$, making it an $\mathbb{R}^{n_x \times m}$ sized matrix</p>

<p>$$ X = \begin{bmatrix} \vdots &amp; \vdots &amp; \vdots &amp; \vdots \newline x^{(1)} &amp; x^{(2)} &amp; \dots &amp; x^{(m)} \newline \vdots &amp; \vdots &amp; \vdots &amp; \vdots \end{bmatrix}  $$</p>

<p>$$ Z = w^TX+b $$</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
<span class="c1"># Z is a row vector of size m</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span></code></pre></div>
<h3 id="vectorizing-logistic-regression-s-gradient-output">Vectorizing Logistic Regression&rsquo;s Gradient Output</h3>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">db</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">m</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dZ</span><span class="p">))</span></code></pre></div>
<h3 id="broadcasting-in-python">Broadcasting in Python</h3>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">56.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">4.4</span><span class="p">,</span> <span class="mf">68.0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">104.0</span><span class="p">,</span> <span class="mf">52.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">135.0</span><span class="p">,</span> <span class="mf">99.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]])</span>
<span class="n">cal</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">cal</span><span class="p">)</span>
<span class="c1"># [59.  239.  155.4  76.9]</span>

<span class="n">percentage</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span><span class="n">A</span><span class="o">/</span><span class="n">cal</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">percentage</span><span class="p">)</span>
<span class="c1">#[[ 94.91525424   0.           2.83140283  88.42652796]</span>
<span class="c1"># [  2.03389831  43.51464435  33.46203346  10.40312094]</span>
<span class="c1"># [  3.05084746  56.48535565  63.70656371   1.17035111]]</span></code></pre></div>
<p>Python does some magic in broadcasting for matrix/array operations:</p>

<p><img src="https://alexander-wong.com/img/deeplearning-ai/python_broadcasting.png" alt="python_broadcasting" /></p>

<h3 id="note-on-python-numpy-vectors">Note on Python/NumPy Vectors</h3>

<p>Broadcasting may introduce subtle bugs in code, as column/row mismatch no longer is thrown</p>

<pre><code>import numpy as np

a = np.random.randn(5) # avoid rank 1 arrays, explicitly define your column vector (5, 1) or row vector (1, 5)
print(a)
# [ 1.2, 2.3, 3.4, 4.5, 5.6 ]
print(a.shape)
# (5,)
print(a.T)
# [ 1.2, 2.3, 3.4, 4.5, 5.6 ]
a = np.random.randn(5, 1)
print(a)
# [[1.2]
#  [2.3]
#  [3.4]
#  [4.5]
#  [5.6]]
</code></pre>

<p>Occastionally assert your shape when you&rsquo;re not sure <code>assert(a.shape == (5, 1))</code>.</p>

<hr />

<p>Move on to <a href="https://alexander-wong.com/post/neural-networks-and-deep-learning-week3/">Week 3</a>.</p>
</article>
    <footer class="post-footer">
      
      <ul class="post-tags">
        
          <li><a href="https://alexander-wong.com/tags/machine-learning"><span class="tag">Machine Learning</span></a></li>
        
          <li><a href="https://alexander-wong.com/tags/deeplearning.ai"><span class="tag">Deeplearning.ai</span></a></li>
        
      </ul>
      
      <p class="post-copyright">
        © 2017 Alexander Wong
      </p>
    </footer>
    
      
    
  </section>
  <footer class="site-footer">
  <p>© 2017 Alexander Wong</p>
  <p>Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a> with theme <a href="https://github.com/laozhu/hugo-nuo" target="_blank">Nuo</a>.</p>
  
</footer>



<script src="//cdn.bootcss.com/video.js/6.2.1/video.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\[','\]']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"] }
    }
  })
</script>
<script src="https://alexander-wong.com/js/bundle.js"></script>


<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-37311284-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>





  </body>
</html>
