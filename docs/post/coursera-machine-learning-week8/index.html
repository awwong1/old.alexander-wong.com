<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8" />

  
  <title>Machine Learning, Week 8</title>

  
  





  
  <meta name="author" content="Alexander Wong" />
  <meta name="description" content="Taking the Coursera Machine Learning course. Will post condensed notes every week as part of the review process. All material originates from the free Coursera course, taught by Andrew Ng.
Assumes you have knowledge of Week 7.
Table of Contents  Unsupervised Learning  Clustering  Introduction K-Means Algorithm Optimization Objective Random Initialization Choosing the Number of Clusters   Dimensionality Reduction  Motivation  Data Compression Visualization  Principal Component Analysis  Principal Component Analysis Problem Formulation Principal Component Analysis Algorithm  Applying PCA  Reconstruction from Compressed Representation Choosing the Number of Principal Components Advice for Applying PCA      Lecture notes:  Lecture13 Lecture14   Unsupervised Learning Clustering Introduction Unsupervised learning is the class of problem solving where when given a set of data with no labels, find structure in the dataset." />

  
  
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@FindingUdia" />
    <meta name="twitter:title" content="Machine Learning, Week 8" />
    <meta name="twitter:description" content="Taking the Coursera Machine Learning course. Will post condensed notes every week as part of the review process. All material originates from the free Coursera course, taught by Andrew Ng.
Assumes you have knowledge of Week 7.
Table of Contents  Unsupervised Learning  Clustering  Introduction K-Means Algorithm Optimization Objective Random Initialization Choosing the Number of Clusters   Dimensionality Reduction  Motivation  Data Compression Visualization  Principal Component Analysis  Principal Component Analysis Problem Formulation Principal Component Analysis Algorithm  Applying PCA  Reconstruction from Compressed Representation Choosing the Number of Principal Components Advice for Applying PCA      Lecture notes:  Lecture13 Lecture14   Unsupervised Learning Clustering Introduction Unsupervised learning is the class of problem solving where when given a set of data with no labels, find structure in the dataset." />
    <meta name="twitter:image" content="https://alexander-wong.com/img/avatar.jpg" />
  




<meta name="generator" content="Hugo 0.31.1" />


<link rel="canonical" href="https://alexander-wong.com/post/coursera-machine-learning-week8/" />
<link rel="alternative" href="https://alexander-wong.com/index.xml" title="Alexander Wong" type="application/atom+xml" />


<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<meta name="format-detection" content="telephone=no,email=no,adress=no" />
<meta http-equiv="Cache-Control" content="no-transform" />


<meta name="robots" content="index,follow" />
<meta name="referrer" content="origin-when-cross-origin" />







<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="apple-mobile-web-app-title" content="Alexander Wong" />
<meta name="msapplication-tooltip" content="Alexander Wong" />
<meta name='msapplication-navbutton-color' content="#5fbf5e" />
<meta name="msapplication-TileColor" content="#5fbf5e" />
<meta name="msapplication-TileImage" content="/img/tile-image-windows.png" />
<link rel="icon" href="https://alexander-wong.com/img/favicon.ico" />
<link rel="icon" type="image/png" sizes="16x16" href="https://alexander-wong.com/img/favicon-16x16.png" />
<link rel="icon" type="image/png" sizes="32x32" href="https://alexander-wong.com/img/favicon-32x32.png" />
<link rel="icon" sizes="192x192" href="https://alexander-wong.com/img/touch-icon-android.png" />
<link rel="apple-touch-icon" href="https://alexander-wong.com/img/touch-icon-apple.png" />
<link rel="mask-icon" href="https://alexander-wong.com/img/safari-pinned-tab.svg" color="#5fbf5e" />



<link rel="stylesheet" href="//cdn.bootcss.com/video.js/6.2.8/alt/video-js-cdn.min.css" />

<link rel="stylesheet" href="https://alexander-wong.com/css/bundle.css" />


  
  <!--[if lt IE 9]>
    <script src="//cdn.bootcss.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="//cdn.bootcss.com/respond.js/1.4.2/respond.min.js"></script>
    <script src="//cdn.bootcss.com/video.js/6.2.8/ie8/videojs-ie8.min.js"></script>
  <![endif]-->

<!--[if lte IE 11]>
    <script src="//cdn.bootcss.com/classlist/1.1.20170427/classList.min.js"></script>
  <![endif]-->


<script src="//cdn.bootcss.com/object-fit-images/3.2.3/ofi.min.js"></script>


<script src="//cdn.bootcss.com/smooth-scroll/12.1.4/js/smooth-scroll.polyfills.min.js"></script>


</head>
  <body>
    
    <div class="suspension">
      <a title="Go to top" class="to-top is-hide"><span class="icon icon-up"></span></a>
      
        
      
    </div>
    
    
  <header class="site-header">
  <img class="avatar" src="https://alexander-wong.com/img/avatar.png" alt="Avatar">
  
  <h2 class="title">Alexander Wong</h2>
  
  <p class="subtitle">Incremental iterations towards meaning in life.</p>
  <button class="menu-toggle" type="button">
    <span class="icon icon-menu"></span>
  </button>
  <nav class="site-menu collapsed">
    <h2 class="offscreen">Main Menu</h2>
    <ul class="menu-list">
      
      
      
      
        <li class="menu-item
            
            
            ">
            <a href="https://alexander-wong.com/">Blog</a>
          </li>
      
        <li class="menu-item
            
            
            ">
            <a href="https://alexander-wong.com/about/">About</a>
          </li>
      
        <li class="menu-item
            
            
            ">
            <a href="https://alexander-wong.com/projects/">Projects</a>
          </li>
      
        <li class="menu-item
            
            
            ">
            <a href="https://alexander-wong.com/chatbot/">Chatbot</a>
          </li>
      
    </ul>
  </nav>
  <nav class="social-menu collapsed">
    <h2 class="offscreen">Social Networks</h2>
    <ul class="social-list">

      
      <li class="social-item">
        <a href="mailto:admin@alexander-wong.com" title="Email"><span class="icon icon-email"></span></a>
      </li>

      
      <li class="social-item">
        <a href="//github.com/awwong1" title="GitHub"><span class="icon icon-github"></span></a>
      </li>

      <li class="social-item">
        <a href="//twitter.com/FindingUdia" title="Twitter"><span class="icon icon-twitter"></span></a>
      </li>

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <li class="social-item">
        <a href="//www.linkedin.com/in/awwong1" title="Linkedin"><span class="icon icon-linkedin"></span></a>
      </li>

      

      

      

      <li class="social-item">
        <a href="https://alexander-wong.com/index.xml"><span class="icon icon-rss" title="RSS"></span></a>
      </li>

    </ul>
  </nav>
</header>

  <section class="main post-detail">
    <header class="post-header">
      <h1 class="post-title">Machine Learning, Week 8</h1>
      <p class="post-meta">@Alexander Wong · Oct 14, 2017 · 6 min read</p>
    </header>
    <article class="post-content">

<p>Taking the <a href="https://www.coursera.org/learn/machine-learning">Coursera Machine Learning</a> course. Will post condensed notes every week as part of the review process. All material originates from the free Coursera course, taught by <a href="http://www.andrewng.org/">Andrew Ng</a>.</p>

<p>Assumes you have knowledge of <a href="https://alexander-wong.com/post/coursera-machine-learning-week7/">Week 7</a>.</p>

<h1>Table of Contents</h1>
<nav id="TableOfContents">
<ul>
<li><a href="#unsupervised-learning">Unsupervised Learning</a>
<ul>
<li><a href="#clustering">Clustering</a>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#k-means-algorithm">K-Means Algorithm</a></li>
<li><a href="#optimization-objective">Optimization Objective</a></li>
<li><a href="#random-initialization">Random Initialization</a></li>
<li><a href="#choosing-the-number-of-clusters">Choosing the Number of Clusters</a></li>
</ul></li>
</ul></li>
<li><a href="#dimensionality-reduction">Dimensionality Reduction</a>
<ul>
<li><a href="#motivation">Motivation</a>
<ul>
<li><a href="#data-compression">Data Compression</a></li>
<li><a href="#visualization">Visualization</a></li>
</ul></li>
<li><a href="#principal-component-analysis">Principal Component Analysis</a>
<ul>
<li><a href="#principal-component-analysis-problem-formulation">Principal Component Analysis Problem Formulation</a></li>
<li><a href="#principal-component-analysis-algorithm">Principal Component Analysis Algorithm</a></li>
</ul></li>
<li><a href="#applying-pca">Applying PCA</a>
<ul>
<li><a href="#reconstruction-from-compressed-representation">Reconstruction from Compressed Representation</a></li>
<li><a href="#choosing-the-number-of-principal-components">Choosing the Number of Principal Components</a></li>
<li><a href="#advice-for-applying-pca">Advice for Applying PCA</a></li>
</ul></li>
</ul></li>
</ul>
</nav>


<ul>
<li>Lecture notes:

<ul>
<li><a href="https://alexander-wong.com/docs/coursera-machine-learning-week8/Lecture13.pdf">Lecture13</a></li>
<li><a href="https://alexander-wong.com/docs/coursera-machine-learning-week8/Lecture14.pdf">Lecture14</a></li>
</ul></li>
</ul>

<h1 id="unsupervised-learning">Unsupervised Learning</h1>

<h2 id="clustering">Clustering</h2>

<h3 id="introduction">Introduction</h3>

<p>Unsupervised learning is the class of problem solving where when given a set of data with no labels, find structure in the dataset.</p>

<p><img src="https://alexander-wong.com/img/coursera-machine-learning-week8/unsupervised_learning.png" alt="unsupervised_learning" /></p>

<p>Clustering is good for problems like:</p>

<ul>
<li>Market segmentation (Create groups for your potential customers)</li>
<li>Social Network analysis (analyze groups of friends)</li>
<li>Organize computing clusters (arrange servers in idea locations to one another)</li>
<li>Astronomical data analysis (Understand groupings of stars)</li>
</ul>

<h3 id="k-means-algorithm">K-Means Algorithm</h3>

<p>This is a clustering algorithm.</p>

<ol>
<li>Randomly initialize your cluster centroids.</li>
<li>Cluster assignment step: Assign each example to a cluster centroid based on distance.</li>
<li>Move centroid step: Take the centroids, move them to the average of all the assigned examples.</li>
<li>Iterate through step 2 and 3 until convergence.</li>
</ol>

<p><img src="https://alexander-wong.com/img/coursera-machine-learning-week8/k_means_1.png" alt="k_means_1" />
<img src="https://alexander-wong.com/img/coursera-machine-learning-week8/k_means_2.png" alt="k_means_2" />
<img src="https://alexander-wong.com/img/coursera-machine-learning-week8/k_means_3.png" alt="k_means_3" />
<img src="https://alexander-wong.com/img/coursera-machine-learning-week8/k_means_4.png" alt="k_means_4" />
<img src="https://alexander-wong.com/img/coursera-machine-learning-week8/k_means_5.png" alt="k_means_5" /></p>

<p><strong>Formal Definition</strong></p>

<p>Input:</p>

<ul>
<li>K (number of clusters)</li>
<li>Training set $ \{ x^{(1)}, x^{(2)}, \dots, x^{(m)} \}$</li>
</ul>

<p>$ x^{(i)} \in \mathbb{R}^n $ (drop $x_0 = 1$ convention)</p>

<p>Randomly initialize $K$ cluster centroids $\mu_1, \mu_2, \dots, \mu_k \in \mathbb{R}^{n}$</p>

<p>Repeat {</p>

<ul>
<li>Cluster Assignment Step

<ul>
<li>for $i = 1$ to $m$, $c^{(i)} :=$ index (from $1$ to $K$) of cluster centroid closest to $x^{(i)}$</li>
</ul></li>
<li>Move Centroid Step

<ul>
<li>for $k = 1 \text{to} K$, $\mu_k :=$ average (mean) of points assigned to cluster $k$</li>
</ul></li>
</ul>

<p>}</p>

<p>If there are clusters with no points assigned to it, it is common practice to remove that cluster. Alternatively, one may reinitialize the algorithm with new cluster centroids.</p>

<h3 id="optimization-objective">Optimization Objective</h3>

<p>The K-Means cost function (optimization objective) is defined here:</p>

<p>$$ c^{(i)} = \text{ index of cluster (1, 2,}\dots, K\text{) to which example } x^{(i)} \text{ is currently assigned} $$
$$ \mu_k = \text{ cluster centroid } k \hspace{1em} (\mu_k \in \mathbb{R}^{n}) $$
$$ \mu_{c^{(i)}} = \text{ cluster centroid of cluster to which example } x^{(i)} \text{ has been assigned} $$</p>

<p>$$ J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots, \mu_K) = \dfrac{1}{m} \sum\limits_{i=1}^{m} || x^{(i)} - \mu_{c^{(i)}} || ^2 $$
$$ \min\limits_{c^{(1)}, \dots, c^{(m)}, \mu_1, \dots, \mu_K} J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots, \mu_K) $$</p>

<h3 id="random-initialization">Random Initialization</h3>

<p>How do you initialize the cluster centroids?</p>

<ul>
<li>Pick a number of clusters less than the number of examples you have

<ul>
<li>should have $K \lt m$</li>
</ul></li>
<li>Randomly pick $K$ training examples</li>
<li>Set $\mu_1, \dots, \mu_K$ equal to these $K$ examples</li>
</ul>

<p><img src="https://alexander-wong.com/img/coursera-machine-learning-week8/k_means_local_optima.png" alt="k_means_local_optima" /></p>

<p>The K-Means algorithm may end up in local optima. The way to get around this is to run K-Means multiple times with multiple random initializations. A typical number of times to run K-Means is 50 - 1000 times. Compute the cost function J and pick the clustering that gives the lowest cost.</p>

<h3 id="choosing-the-number-of-clusters">Choosing the Number of Clusters</h3>

<p>Choosing the number of clusters $K$ in K-means is a non trivial problem as clusters may or may not be intuitive. Usually, it is still a manual step where an individual picks the number of clusters by looking at a plot.</p>

<p><img src="https://alexander-wong.com/img/coursera-machine-learning-week8/num_clusters.png" alt="num_clusters" /></p>

<ul>
<li>are there 2, 3, or 4 clusters? It&rsquo;s ambiguous</li>
</ul>

<p><strong>Elbow Method</strong></p>

<ul>
<li>Run K-Means with varying number of clusters. (1 cluster, then 2 clusters, then 3&hellip; so on and so on)</li>
<li>Ends up with a curve showing how distortion decreases as the number of clusters increases</li>
</ul>

<p><img src="https://alexander-wong.com/img/coursera-machine-learning-week8/elbow_method.png" alt="elbow_method" /></p>

<ul>
<li>usually the &lsquo;elbow&rsquo; is not clearly defined</li>
</ul>

<p>Usually K-Means is downstream purpose specific. For example, when calculating clusters for market segmentation, if we are selling T-Shirts, perhaps it is more useful to have pre-defined clusters &ldquo;Small, Medium, Large&rdquo; or &ldquo;Extra Small, Medium, Large, Extra Large&rdquo; sizes.</p>

<h1 id="dimensionality-reduction">Dimensionality Reduction</h1>

<h2 id="motivation">Motivation</h2>

<h3 id="data-compression">Data Compression</h3>

<p>There are two primary reasons to perform dimensionality reduction. One of them is data compression, and the other is that dimensionality reduction can increase performance of our learning algorithms.</p>

<p>Given a two features like length in inches and centemeters, with slight roundoff error, there is a lot of redundancy. It would be useful to convert a 2D plot into a 1D vector.</p>

<p><img src="https://alexander-wong.com/img/coursera-machine-learning-week8/data_compression.png" alt="data_compression" /></p>

<p>Before, we needed two numbers to represent an example. After compression, only one number is necessary to represent the example.</p>

<p>The typical example of dimensionality reduction is from 1000D to 100D.</p>

<p><img src="https://alexander-wong.com/img/coursera-machine-learning-week8/3D_data_compression.png" alt="3D_data_compression" /></p>

<h3 id="visualization">Visualization</h3>

<p>Dimensionality Reduction also helps us visualize the data better. Suppose we have the following dataset:</p>

<p><img src="https://alexander-wong.com/img/coursera-machine-learning-week8/visualization_dataset.png" alt="visualization_dataset" /></p>

<p>We want to reduce the features to a two or three dimensional vector in order to better understand the data, rather than attempt to plot a 50 dimension table.</p>

<p><img src="https://alexander-wong.com/img/coursera-machine-learning-week8/visualization_analysis.png" alt="visualization_analysis" /></p>

<h2 id="principal-component-analysis">Principal Component Analysis</h2>

<h3 id="principal-component-analysis-problem-formulation">Principal Component Analysis Problem Formulation</h3>

<p>PCA is the most popular algorithm to perform dimensionality reduction.</p>

<ul>
<li>Find a lower dimensional surface such that the sum of squares error (projection error) is minimized.</li>
<li>Standard practice is to perform feature scalining and mean normalization before scaling the data.</li>
</ul>

<p><img src="https://alexander-wong.com/img/coursera-machine-learning-week8/pca_problem_formulation.png" alt="pca_problem_formulation" /></p>

<p><strong>PCA is not linear regression</strong></p>

<p><img src="https://alexander-wong.com/img/coursera-machine-learning-week8/pca_not_linear_regression.png" alt="pca_not_linear_regression" /></p>

<ul>
<li>In linear regression, we are minimizing the point and the value predicted b the hypothesis</li>
<li>In PCA, we are minimizing the distance between the point and the line</li>
</ul>

<h3 id="principal-component-analysis-algorithm">Principal Component Analysis Algorithm</h3>

<ol>
<li><p>Data preprocessing step:</p>

<ul>
<li>Given your training set $x^{(1)}, x^{(2)}, \dots, x^{(m)}$</li>
<li>Perform feature scaling and mean normalization
$$ \mu_j = \dfrac{1}{m} \sum\limits_{i=1}^m x_j^{(i)} $$</li>
<li>Replace each $x_j^{(i)}$ with $ x_j - \mu_j $.</li>
<li>If different features on different scales, (e.g. size of house, number of bedrooms) scale features to have comparable range of values.
$$ x_j^{(i)} \leftarrow \dfrac{x_j^{(i)} - \mu_j}{s_j} $$</li>
</ul></li>

<li><p>PCA algorithm</p>

<ul>
<li>Reduce data from $n$-dimensions to $k$-dimensions</li>
<li>Compute the &ldquo;covariance matrix&rdquo;:</li>
<li>(it is unfortunate that the Sigma value is used, do not confuse with summation)
$$ \Sigma = \dfrac{1}{m} \sum\limits_{i=1}^{n} (x^{(i)})(x^{(i)})^T $$</li>
<li>Compute the &lsquo;eigenvectors&rsquo; of matrix $\Sigma$</li>
</ul></li>
</ol>
<div class="highlight"><pre class="chroma"><code class="language-octave" data-lang="octave">  <span class="p">[</span> <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="p">]</span> <span class="p">=</span> <span class="nb">svd</span><span class="p">(</span><span class="n">Sigma</span><span class="p">)</span><span class="err">
</span><span class="err"></span>  <span class="c">% svd stands for singular value decomposition</span><span class="err">
</span><span class="err"></span>  <span class="c">% another function that does this is the eig(Sigma) function</span><span class="err">
</span><span class="err"></span>  <span class="c">% svd(Sigma) returns an n * n matrix</span></code></pre></div>
<p><img src="https://alexander-wong.com/img/coursera-machine-learning-week8/eigenvectors_to_pca.png" alt="eigenvectors_to_pca" /></p>

<p><img src="https://alexander-wong.com/img/coursera-machine-learning-week8/pca_algorithm.png" alt="pca_algorithm" /></p>

<h2 id="applying-pca">Applying PCA</h2>

<h3 id="reconstruction-from-compressed-representation">Reconstruction from Compressed Representation</h3>

<p>After compression, how do we go back to the higher dimensional state?</p>

<p><img src="https://alexander-wong.com/img/coursera-machine-learning-week8/pca_reconstruction.png" alt="pca_reconstruction" /></p>

<h3 id="choosing-the-number-of-principal-components">Choosing the Number of Principal Components</h3>

<p>In the PCA algorithm, how do we choose the value for $k$? How do we choose the number of principal components?</p>

<p>Recall that PCA tries to minimize the average squared projection error</p>

<p>$$ \dfrac{1}{m} \sum_{i=1}^{m} || x^{(i)} - x_{\text{approx}}^{(i)} ||^2 $$</p>

<p>Also, the total variation in the data is defined as</p>

<p>$$ \dfrac{1}{m} \sum_{i=1}^{m} || x^{(i)} || ^2 $$</p>

<p>Typically choose $k$ to be the smallest value such that</p>

<p>$$ \dfrac{ \dfrac{1}{m} \sum_{i=1}^{m} || x^{(i)} - x_{\text{approx}}^{(i)} || ^ 2 }{ \dfrac{1}{m} \sum_{i=1}^{m} || x^{(i)} || ^ 2 } \leq 0.01 $$
$$ \text{This means that 99% of variance is retained.} $$</p>

<p>Vary the percentage between 95-99 percent, depending on your application and requirements.</p>

<p>This is how to calculate the variance using the <code>svd(Sigma)</code> function return values:</p>

<p><img src="https://alexander-wong.com/img/coursera-machine-learning-week8/calculate_pca_variance.png" alt="calculate_pca_variance" /></p>

<h3 id="advice-for-applying-pca">Advice for Applying PCA</h3>

<p>PCA can be applied to reduce your training set dimensions before feeding the resulting training set to a learning algorithm.</p>

<p>Only run PCA on the training set. Do not run PCA on the cross validation and test sets.</p>

<p>One bad use of PCA is to prevent overfitting. Use regularization instead. PCA throws away some information without knowing what the corresponding values of y are.</p>

<p>Do not unnecessarily run PCA. It is valid to run your learning algorithms using the raw data $x^{(i)}$ and only when that fails, implement PCA.</p>

<hr />

<p>Move on to <a href="https://alexander-wong.com/post/coursera-machine-learning-week9/">Week 9</a>.</p>
</article>
    <footer class="post-footer">
      
      <ul class="post-tags">
        
          <li><a href="https://alexander-wong.com/tags/machine-learning"><span class="tag">Machine Learning</span></a></li>
        
      </ul>
      
      <p class="post-copyright">
        © 2017 Alexander WongThis post was published <strong>120</strong> days ago, content in the post may be inaccurate, even wrong now, please take risk yourself.
      </p>
    </footer>
    
      
    
  </section>
  <footer class="site-footer">
  <p>© 2017-2018 Alexander Wong</p>
  <p>Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a> with theme <a href="https://github.com/laozhu/hugo-nuo" target="_blank">Nuo</a>.</p>
  
</footer>



<script src="//cdn.bootcss.com/video.js/6.2.1/video.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\[','\]']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"] }
    }
  })
</script>
<script src="https://alexander-wong.com/js/bundle.js"></script>


<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-37311284-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>





  </body>
</html>
