<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8" />

  
  <title>Machine Learning, Week 7</title>

  
  
  <link href="//cdn.jsdelivr.net" rel="dns-prefetch">
  <link href="//cdnjs.cloudflare.com" rel="dns-prefetch">
  <link href="//at.alicdn.com" rel="dns-prefetch">
  <link href="//fonts.googleapis.com" rel="dns-prefetch">
  <link href="//fonts.gstatic.com" rel="dns-prefetch">
  
  
  
  <link href="//www.google-analytics.com" rel="dns-prefetch">
  

  

  
  <meta name="author" content="Alexander Wong">
  <meta name="description" content="Taking the Coursera Machine Learning course. Will post condensed notes every week as part of the review process. All material originates from the free Coursera course, taught by Andrew Ng.
Assumes you have knowledge of Week 6.
Table of Contents  Support Vector Machines  Large Margin Classification  Optimization Objective Large Margin Intuition  Kernels Source Vector Machines (in Practice)     Lecture notes:  Lecture12   Support Vector Machines Large Margin Classification Optimization Objective We are simplifying the logistic regression cost function by converting the sigmoid function into two straight lines, as shown here:">

  
  
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@FindingUdia">
    <meta name="twitter:title" content="Machine Learning, Week 7">
    <meta name="twitter:description" content="Taking the Coursera Machine Learning course. Will post condensed notes every week as part of the review process. All material originates from the free Coursera course, taught by Andrew Ng.
Assumes you have knowledge of Week 6.
Table of Contents  Support Vector Machines  Large Margin Classification  Optimization Objective Large Margin Intuition  Kernels Source Vector Machines (in Practice)     Lecture notes:  Lecture12   Support Vector Machines Large Margin Classification Optimization Objective We are simplifying the logistic regression cost function by converting the sigmoid function into two straight lines, as shown here:">
    <meta name="twitter:image" content="/img/avatar.png">
  

  
  <meta property="og:type" content="article">
  <meta property="og:title" content="Machine Learning, Week 7">
  <meta property="og:description" content="Taking the Coursera Machine Learning course. Will post condensed notes every week as part of the review process. All material originates from the free Coursera course, taught by Andrew Ng.
Assumes you have knowledge of Week 6.
Table of Contents  Support Vector Machines  Large Margin Classification  Optimization Objective Large Margin Intuition  Kernels Source Vector Machines (in Practice)     Lecture notes:  Lecture12   Support Vector Machines Large Margin Classification Optimization Objective We are simplifying the logistic regression cost function by converting the sigmoid function into two straight lines, as shown here:">
  <meta property="og:url" content="https://old.alexander-wong.com/post/coursera-machine-learning-week7/">
  <meta property="og:image" content="/img/avatar.png">




<meta name="generator" content="Hugo 0.53">


<link rel="canonical" href="https://old.alexander-wong.com/post/coursera-machine-learning-week7/">

<meta name="renderer" content="webkit">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="format-detection" content="telephone=no,email=no,adress=no">
<meta http-equiv="Cache-Control" content="no-transform">


<meta name="robots" content="index,follow">
<meta name="referrer" content="origin-when-cross-origin">







<meta name="theme-color" content="#02b875">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta name="apple-mobile-web-app-title" content="Alexander Wong">
<meta name="msapplication-tooltip" content="Alexander Wong">
<meta name='msapplication-navbutton-color' content="#02b875">
<meta name="msapplication-TileColor" content="#02b875">
<meta name="msapplication-TileImage" content="/icons/icon-144x144.png">
<link rel="icon" href="https://old.alexander-wong.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://old.alexander-wong.com/icons/icon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://old.alexander-wong.com/icons/icon-32x32.png">
<link rel="icon" sizes="192x192" href="https://old.alexander-wong.com/icons/icon-192x192.png">
<link rel="apple-touch-icon" href="https://old.alexander-wong.com/icons/icon-152x152.png">
<link rel="manifest" href="https://old.alexander-wong.com/manifest.json">


<link rel="preload" href="https://old.alexander-wong.com/styles/main.min.css" as="style">
<link rel="preload" href="https://fonts.googleapis.com/css?family=Lobster" as="style">
<link rel="preload" href="https://old.alexander-wong.com/img/avatar.png" as="image">
<link rel="preload" href="https://old.alexander-wong.com/images/grey-prism.svg" as="image">


<style>
  body {
    background: rgb(244, 243, 241) url('/images/grey-prism.svg') repeat fixed;
  }
</style>
<link rel="stylesheet" href="https://old.alexander-wong.com/styles/main.min.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lobster">



<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.2/dist/medium-zoom.min.js"></script>




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/video.js@7.3.0/dist/video-js.min.css">



  
  
<!--[if lte IE 8]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/videojs-ie8@1.1.2/dist/videojs-ie8.min.js"></script>
<![endif]-->

<!--[if lte IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/eligrey-classlist-js-polyfill@1.2.20180112/classList.min.js"></script>
<![endif]-->


</head>
  <body>
    
    <div class="suspension">
      <a role="button" aria-label="Go to top" title="Go to top" class="to-top is-hide"><span class="icon icon-up" aria-hidden="true"></span></a>
      
        
      
    </div>
    
    
  <header class="site-header">
  <img class="avatar" src="https://old.alexander-wong.com/img/avatar.png" alt="Avatar">
  
  <h2 class="title">Alexander Wong</h2>
  
  <p class="subtitle">Incremental iterations towards meaning in life.</p>
  <button class="menu-toggle" type="button" aria-label="Main Menu" aria-expanded="false" tab-index="0">
    <span class="icon icon-menu" aria-hidden="true"></span>
  </button>

  <nav class="site-menu collapsed">
    <h2 class="offscreen">Main Menu</h2>
    <ul class="menu-list">
      
      
      
      
        <li class="menu-item
          
          
          ">
          <a href="https://old.alexander-wong.com/">Blog</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="https://old.alexander-wong.com/about/">About</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="https://old.alexander-wong.com/projects/">Projects</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="https://old.alexander-wong.com/chatbot/">Chatbot</a>
        </li>
      
    </ul>
  </nav>
  <nav class="social-menu collapsed">
    <h2 class="offscreen">Social Networks</h2>
    <ul class="social-list"><li class="social-item">
          <a href="mailto:admin@alexander-wong.com" title="Email" aria-label="Email">
            <span class="icon icon-email" aria-hidden="true"></span>
          </a>
        </li><li class="social-item">
          <a href="//github.com/awwong1" title="GitHub" aria-label="GitHub">
            <span class="icon icon-github" aria-hidden="true"></span>
          </a>
        </li><li class="social-item">
          <a href="//twitter.com/FindingUdia" title="Twitter" aria-label="Twitter">
            <span class="icon icon-twitter" aria-hidden="true"></span>
          </a>
        </li><li class="social-item">
          <a href="//www.linkedin.com/in/awwong1" title="Linkedin" aria-label="Linkedin">
            <span class="icon icon-linkedin" aria-hidden="true"></span>
          </a>
        </li></ul>
  </nav>
</header>

  <section class="main post-detail">
    <header class="post-header">
      <h1 class="post-title">Machine Learning, Week 7</h1>
      <p class="post-meta">@Alexander Wong · Oct 4, 2017 · 4 min read</p>
    </header>
    <article class="post-content">

<p>Taking the <a href="https://www.coursera.org/learn/machine-learning">Coursera Machine Learning</a> course. Will post condensed notes every week as part of the review process. All material originates from the free Coursera course, taught by <a href="http://www.andrewng.org/">Andrew Ng</a>.</p>

<p>Assumes you have knowledge of <a href="https://old.alexander-wong.com/post/coursera-machine-learning-week6/">Week 6</a>.</p>

<h1>Table of Contents</h1>
<nav id="TableOfContents">
<ul>
<li><a href="#support-vector-machines">Support Vector Machines</a>
<ul>
<li><a href="#large-margin-classification">Large Margin Classification</a>
<ul>
<li><a href="#optimization-objective">Optimization Objective</a></li>
<li><a href="#large-margin-intuition">Large Margin Intuition</a></li>
</ul></li>
<li><a href="#kernels">Kernels</a></li>
<li><a href="#source-vector-machines-in-practice">Source Vector Machines (in Practice)</a></li>
</ul></li>
</ul>
</nav>


<ul>
<li>Lecture notes:

<ul>
<li><a href="https://old.alexander-wong.com/docs/coursera-machine-learning-week7/Lecture12.pdf">Lecture12</a></li>
</ul></li>
</ul>

<h1 id="support-vector-machines">Support Vector Machines</h1>

<h2 id="large-margin-classification">Large Margin Classification</h2>

<h3 id="optimization-objective">Optimization Objective</h3>

<p>We are simplifying the logistic regression cost function by converting the sigmoid function into two straight lines, as shown here:</p>

<p><img src="https://old.alexander-wong.com/img/coursera-machine-learning-week7/svm_cost1_cost0.png" alt="svm_cost1_cost0" /></p>

<p>The following are two cost functions for support vector machines:</p>

<p>$$ \min\limits_{\theta} \dfrac{1}{m} [\sum\limits_{i=1}^m y^{(i)} \text{cost}_1(\theta^Tx^{(i)}) + (1-y^{(i)}) \text{cost}_0(\theta^Tx^{(i)}) ] + \dfrac{\lambda}{2m} \sum\limits_{j=1}^n \theta_j^2 $$
$$ \min\limits_{\theta} C[ \sum\limits_{i=1}^m y^{(i)} \text{cost}_1(\theta^Tx^{(i)}) + (1-y^{(i)}) \text{cost}_0(\theta^Tx^{(i)}) ] + \dfrac{1}{2} \sum\limits_{j=1}^{n} \theta^2_j  $$</p>

<p>They both give the same value of $\theta$ if $C = \dfrac{1}{\lambda} $.</p>

<p>Hypothesis will predict:</p>

<p>$$ h_\theta(x) = 1 \hspace{1em} \text{if} \hspace{1em} \theta^Tx \geq 0 $$
$$ h_\theta(x) = 0 \hspace{1em} \text{otherwise} $$</p>

<h3 id="large-margin-intuition">Large Margin Intuition</h3>

<p>Support Vector Machines are also known as Large Margin Classifiers. This is because when plotting the positive and negative examples, a support vector machine will draw a decision boundary with large margins:</p>

<p><img src="https://old.alexander-wong.com/img/coursera-machine-learning-week7/large_margin_classifier.png" alt="large_margin_classifier" /></p>

<p>This is different than linear regression, where the decision boundary can be very close to the positive and negative examples (due to $\theta^Tx \approx 0$ in the $y=1 \text{ or } y=0$ cases. )</p>

<p><img src="https://old.alexander-wong.com/img/coursera-machine-learning-week7/svm_vs_linear_regression.png" alt="svm_vs_linear_regression" /></p>

<p>When the data is not linearly sepearable, one should take into consideration the regularization parameter $C$.</p>

<p><img src="https://old.alexander-wong.com/img/coursera-machine-learning-week7/svm_outliers.png" alt="svm_outliers" /></p>

<ul>
<li>The magenta line is when the regularization parameter is not large and there is a the one small outlier in the bottom left corner.</li>
<li>The black line is when the regularization parameter is large.</li>
<li>The black line could also be when the regularization parameter is small and there are many datapoints (the drawn X&rsquo;s and O&rsquo;s) making the plot difficult to separate linearly.</li>
</ul>

<h2 id="kernels">Kernels</h2>

<p>In a non linear decision boundary, we can have many choices for high order polynomials. A Kernel is, given $x$, compute a new feature depending on proximity to landmarks $l^{(1)}, l^{(2)}, l^{(3)} $</p>

<p>Given the example $x$:
$$ f_1 = \text{ similarity}(x, l^{(1)}) = \exp(-\dfrac{|| x - l^{(1)} ||^2}{2 \sigma ^2}) $$
$$ f_2 = \text{ similarity}(x, l^{(2)}) = \exp(-\dfrac{|| x - l^{(2)} ||^2}{2 \sigma ^2}) $$
$$ || x - l^{(1)} ||^2 = \text{ square of the euclidian distance between x and l}^{(1)} $$</p>

<p>These functions are kernels, these specific ones are gaussian kernels. Consider them similarity functions.</p>

<p>If $x \approx l^{(1)}$ then $ f_1 \approx \exp(-\dfrac{0^2}{2\sigma^2}) \approx 1 $.</p>

<p>If $x$ is far from $l^{(1)}$ then $ f_1 = \exp(-\dfrac{\text{large number}^2}{2\sigma^2}) \approx 0 $</p>

<p>The smaller the sigma, the feature falls more rapidly.</p>

<p><img src="https://old.alexander-wong.com/img/coursera-machine-learning-week7/kernel_sigma.png" alt="kernel_sigma" /></p>

<p>How do we choose the landmarks $l$?</p>

<p>Given $m$ training examples, set $l$ to be each one of your training examples.</p>

<p><img src="https://old.alexander-wong.com/img/coursera-machine-learning-week7/kernel_landmarks.png" alt="kernel_landmarks" /></p>

<p>The following is how you would train using kernels (similarity functions):</p>

<p><img src="https://old.alexander-wong.com/img/coursera-machine-learning-week7/kernel_training.png" alt="kernel_training" /></p>

<p>When using an SVM, one of the choices that need to be made is $C$. Also, one must consider the choice of $\sigma^2$. The following is the bias/variance tradeoff diagrams.</p>

<p><img src="https://old.alexander-wong.com/img/coursera-machine-learning-week7/kernel_bias_variance_tradeoff.png" alt="kernel_bias_variance_tradeoff" /></p>

<h2 id="source-vector-machines-in-practice">Source Vector Machines (in Practice)</h2>

<p>When to choose between a linear kernel and a gaussian kernel?</p>

<p><img src="https://old.alexander-wong.com/img/coursera-machine-learning-week7/which_kernel_to_use.png" alt="which_kernel_to_use" /></p>

<p>Note: When using the Gaussian kernel, it is important to perform feature scaling beforehand.</p>

<p>The kernels that you choose must satisfy a technical condition called &ldquo;Mercer&rsquo;s Theorem&rdquo; to make sure SVM packages&rsquo; optimizations run correctly and do not diverge.</p>

<ul>
<li>Polynomial kernel: $ k(x,l) \in { (x^Tl)^2, (x^Tl)^3, (x^Tl + 7)^7 } $

<ul>
<li>usually performs worse than the gaussian kernel</li>
</ul></li>
<li>String kernel, chi-square kernel, histogram intersection kernel, etc.</li>
</ul>

<p><strong>Logistic regression vs Source Vector Machines</strong></p>

<ul>
<li>If $n$ is large relative to $m$ (n = 10,000, m=10-1000)

<ul>
<li>use logistic regression, or SVM without a kernel (&ldquo;linear kernel&rdquo;)</li>
</ul></li>
<li>If $n$ is small and $m$ is intermediate (n = 1-1000, m = 10-10,000)

<ul>
<li>use SVM with Gaussian kernel</li>
</ul></li>
<li>If n is small and m is large (n = 1-1000, m = 50000+)

<ul>
<li>Create/add more features, then use logistic regression or SVM without a kernel.</li>
</ul></li>
</ul>

<p>Neural networks are likely to work well for msot of these settings, but may be slower to train.</p>

<hr />

<p>Move on to <a href="https://old.alexander-wong.com/post/coursera-machine-learning-week8/">Week 8</a>.</p>
</article>
    <footer class="post-footer">
      
      <ul class="post-tags">
        
          <li><a href="https://old.alexander-wong.com/tags/machine-learning"><span class="tag">Machine Learning</span></a></li>
        
      </ul>
      
      <p class="post-copyright">
        © 2017 Alexander WongThis post was published <strong>460</strong> days ago, content in the post may be inaccurate, even wrong now, please take risk yourself.
      </p>
    </footer>
    
      
    
  </section>
  <footer class="site-footer">
  <p>© 2017-2019 Alexander Wong</p>
  <p>Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a> with theme <a href="https://github.com/laozhu/hugo-nuo" target="_blank">Nuo</a>.</p>
  
</footer>



<script src="//cdn.bootcss.com/video.js/6.2.1/video.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\[','\]']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"] }
    }
  })
</script>
<script src="https://old.alexander-wong.com/js/bundle.js"></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-37311284-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>





  </body>
</html>
